# -----------------------------------------------
# parameters defining the network architecture
architecture:  MLP-T

input_dim  : 6
output_dim : 50

num_mlp_blocks: 4
mlp_dims: [25, 50, 100, 500, 1000]

# If you aren't using the transformer block, you can disregard these arguments
patch_size             : [17, 5]
num_transformer_blocks : 5
num_heads              : 5
embedding              : True        # <- whether to apply possitional embedding
dropout_prob           : 0.25         
freeze_mlp             : True        # <- whether to keeep mlp weights frozen during training
train_mlp_first        : True

# Training parameters
num_epochs: 30
learning_rate: [1.438e-3, 1.e-4, 1.e-5]
batch_size: 600

train_gaussian_only   : False
start_from_checkpoint : False

# numbers with which to normalize the training set
# Should be updated after generating your specific training set
norm_pos : 5.8111
norm_neg : 4.7667

# cosmology parameter bounds. This should line up with what you used to generate your
# training set
parameter_bounds:
 - [50, 100]
 - [0.02, 0.3]
 - [0.75, 5.]
 - [1, 4]
 - [-4, 4]
 - [-4, 4]

save_dir: /home/u12/jadamo/CovNet/emulators/ngc_z3/MLP-T/
training_dir: /xdisk/timeifler/jadamo/Training-Set-HighZ-NGC/